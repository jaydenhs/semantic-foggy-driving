{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.io import read_image\n",
    "from skimage import io, transform\n",
    "import torchsummary\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class YourCustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_path, input, target, split, input_beta = 0.01, train_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_path (str): path to data\n",
    "            input (str): specific path to input data \n",
    "            target (str): specific path to target (annotation) data\n",
    "            split (str): split to get data from (\"train\", \"test\", or \"val\")\n",
    "            input_beta (float): defines the synthetic fog density to filter the images to\n",
    "            train_transform (bool, optional): Applies the defined data transformations used in training. Defaults to None.\n",
    "        \"\"\"\n",
    "        super(YourCustomDataset, self).__init__()\n",
    "        self.root_path = root_path\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.split = split\n",
    "        self.input_beta = input_beta\n",
    "        self.train_transform = train_transform\n",
    "\n",
    "        # iterates through split of data and creates an array of image names filtered to specified fog beta\n",
    "        self.image_names = []\n",
    "        X_SPLIT_PATH = os.path.join(self.root_path, self.input, self.split)\n",
    "        for CITY_NAME in os.listdir(X_SPLIT_PATH):\n",
    "            CITY_PATH = os.path.join(X_SPLIT_PATH, CITY_NAME)\n",
    "            for image_name in os.listdir(CITY_PATH):\n",
    "                if str(self.input_beta) in image_name:\n",
    "                    IMAGE_PATH = os.path.join(CITY_PATH, image_name)\n",
    "                    self.image_names.append(IMAGE_PATH)\n",
    "\n",
    "        # same for annotation_names, filters to label annotation_names\n",
    "        self.annotation_names = []\n",
    "        Y_SPLIT_PATH = os.path.join(self.root_path, self.target, self.split)\n",
    "        for CITY_NAME in os.listdir(Y_SPLIT_PATH):\n",
    "            CITY_PATH = os.path.join(Y_SPLIT_PATH, CITY_NAME)\n",
    "            for annotation_name in os.listdir(CITY_PATH):\n",
    "                if \"label\" in annotation_name:\n",
    "                    ANNOTATION_PATH = os.path.join(CITY_PATH, annotation_name)\n",
    "                    self.annotation_names.append(ANNOTATION_PATH)\n",
    "\n",
    "    def __len__(self):\n",
    "        number_files_input = len(self.image_names)\n",
    "        number_files_target = len(self.annotation_names)\n",
    "\n",
    "        if number_files_input == number_files_target:\n",
    "            return number_files_input\n",
    "        else:\n",
    "            return f\"Input: {number_files_input} does not match Target: {number_files_target}\"\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Necessary function that loads and returns a sample from the dataset at a given index. \n",
    "        https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "        \n",
    "        Based on the index,it identifies the input and target images location on the disk, \n",
    "        reads both items as a numpy array (float32). If the train_transform argument is True, \n",
    "        the above defined train transformations are applied. Else, the test transformations are applied\n",
    "        Args:\n",
    "            idx (iterable): \n",
    "        Returns:\n",
    "            tensors: input and target image\n",
    "        \"\"\"\n",
    "        input_path = self.image_names[idx]\n",
    "        target_path = self.annotation_names[idx]\n",
    "\n",
    "        input_image = io.imread(input_path)\n",
    "        target_image = io.imread(target_path)\n",
    "\n",
    "        # print(target_image.min(), target_image.max())\n",
    "        # print(f\"There are {len(np.unique(target_image))} unique target values\")\n",
    "\n",
    "        input_image = transform.resize(input_image, (1396, 1396), order=0)\n",
    "        # TO-DO: Map from id (34 options) to trainId (19 classes)\n",
    "        # https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/helpers/labels.py\n",
    "        target_image = transform.resize(target_image, (216, 216), order=0)\n",
    "\n",
    "        input_image = torch.from_numpy(input_image).float() / 255.0\n",
    "        target_image = torch.from_numpy(target_image).long()\n",
    "        \n",
    "        sample = {\n",
    "            'input_image': input_image,\n",
    "            'input_name': input_path.split(\"\\\\\")[-1],\n",
    "            'target_image': target_image,\n",
    "            'target_name': target_path.split(\"\\\\\")[-1]\n",
    "        }\n",
    "\n",
    "        return sample    \n",
    "    \n",
    "def plot_sample(sample):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.subplot(121)\n",
    "    plt.title(sample['input_name'])\n",
    "    plt.imshow(sample['input_image'])\n",
    "    plt.subplot(122)\n",
    "    plt.title(sample['target_name'])\n",
    "    plt.imshow(sample['target_image'], cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "ROOT_PATH = r\"C:\\Users\\Jayden Hsiao\\Documents\\Grad School\\02 - Fall 2024\\SYDE 577\\semantic-foggy-driving\\data\"\n",
    "INPUT_PATH = r\"leftImg8bit_trainvaltest_foggy\\leftImg8bit_foggy\"\n",
    "TARGET_PATH = r\"gtFine_trainvaltest\\gtFine\"\n",
    "\n",
    "train_dataset = YourCustomDataset(root_path=ROOT_PATH, input=INPUT_PATH, target=TARGET_PATH, split=\"train\")\n",
    "val_dataset = YourCustomDataset(root_path=ROOT_PATH, input=INPUT_PATH, target=TARGET_PATH, split=\"val\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# plot_sample(train_dataset[0])\n",
    "# plot_sample(train_dataset[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribed from Caffe to Pytorch\n",
    "# Source: https://github.com/fyu/dilation/blob/master/models/dilation10_cityscapes_deploy.prototxt\n",
    "class Dilation10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dilation10, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc6 = nn.Conv2d(512, 4096, kernel_size=7, padding=4, dilation=4)\n",
    "        self.drop6 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc7 = nn.Conv2d(4096, 4096, kernel_size=1)\n",
    "        self.drop7 = nn.Dropout(0.5)\n",
    "\n",
    "        self.final = nn.Conv2d(4096, 19, kernel_size=1)\n",
    "\n",
    "        self.ctx_conv1_1 = nn.Conv2d(19, 19, kernel_size=3, padding=1)\n",
    "        self.ctx_conv1_2 = nn.Conv2d(19, 19, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.ctx_conv2_1 = nn.Conv2d(19, 19, kernel_size=3, padding=2, dilation=2)\n",
    "        self.ctx_conv3_1 = nn.Conv2d(19, 19, kernel_size=3, padding=4, dilation=4)\n",
    "        self.ctx_conv4_1 = nn.Conv2d(19, 19, kernel_size=3, padding=8, dilation=8)\n",
    "        self.ctx_conv5_1 = nn.Conv2d(19, 19, kernel_size=3, padding=16, dilation=16)\n",
    "        self.ctx_conv6_1 = nn.Conv2d(19, 19, kernel_size=3, padding=32, dilation=32)\n",
    "        self.ctx_conv7_1 = nn.Conv2d(19, 19, kernel_size=3, padding=64, dilation=64)\n",
    "        \n",
    "        self.ctx_fc1 = nn.Conv2d(19, 19, kernel_size=3, padding=1)\n",
    "        self.ctx_final = nn.Conv2d(19, 19, kernel_size=1)\n",
    "\n",
    "        self.ctx_upsample = nn.ConvTranspose2d(19, 19, kernel_size=16, stride=8, padding=4, groups=19, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Block 2\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Block 3\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = F.relu(self.conv3_2(x))\n",
    "        x = F.relu(self.conv3_3(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Block 4\n",
    "        x = F.relu(self.conv4_1(x))\n",
    "        x = F.relu(self.conv4_2(x))\n",
    "        x = F.relu(self.conv4_3(x))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        # Block 5\n",
    "        x = F.relu(self.conv5_1(x))\n",
    "        x = F.relu(self.conv5_2(x))\n",
    "        x = F.relu(self.conv5_3(x))\n",
    "        x = self.pool5(x)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.drop6(x)\n",
    "\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = self.drop7(x)\n",
    "\n",
    "        # Final convolution layers\n",
    "        x = self.final(x)\n",
    "\n",
    "        # Context layers\n",
    "        x = F.relu(self.ctx_conv1_1(x))\n",
    "        x = F.relu(self.ctx_conv1_2(x))\n",
    "\n",
    "        x = F.relu(self.ctx_conv2_1(x))\n",
    "        x = F.relu(self.ctx_conv3_1(x))\n",
    "        x = F.relu(self.ctx_conv4_1(x))\n",
    "        x = F.relu(self.ctx_conv5_1(x))\n",
    "        x = F.relu(self.ctx_conv6_1(x))\n",
    "        x = F.relu(self.ctx_conv7_1(x))\n",
    "\n",
    "        x = F.relu(self.ctx_fc1(x))\n",
    "        x = self.ctx_final(x)\n",
    "\n",
    "        # Upsample\n",
    "        x = self.ctx_upsample(x)\n",
    "\n",
    "        # Softmax\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Select GPU if available\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDilation10\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Move the model to the GPU\u001b[39;00m\n\u001b[0;32m      5\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jayden Hsiao\\Documents\\Grad School\\02 - Fall 2024\\SYDE 577\\semantic-foggy-driving\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jayden Hsiao\\Documents\\Grad School\\02 - Fall 2024\\SYDE 577\\semantic-foggy-driving\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jayden Hsiao\\Documents\\Grad School\\02 - Fall 2024\\SYDE 577\\semantic-foggy-driving\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jayden Hsiao\\Documents\\Grad School\\02 - Fall 2024\\SYDE 577\\semantic-foggy-driving\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1155\u001b[0m             device,\n\u001b[0;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m             non_blocking,\n\u001b[0;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1159\u001b[0m         )\n\u001b[1;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Select GPU if available\n",
    "print(device)\n",
    "\n",
    "model = Dilation10().to(device)  # Move the model to the GPU\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "def train_one_epoch():\n",
    "    running_loss = 0.\n",
    "    loss_values = []  # Store loss values for plotting later\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        if i > 1:\n",
    "            break\n",
    "\n",
    "        # Move inputs and labels to the GPU\n",
    "        inputs = sample['input_image'].permute(0, 3, 1, 2).to(device)\n",
    "        labels = sample['target_image'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero your gradients for every batch!\n",
    "\n",
    "        outputs = model(inputs)  # Make predictions\n",
    "\n",
    "        print(\"inputs:\", inputs.size())\n",
    "        print(\"labels:\", labels.size())\n",
    "        print(\"outputs:\", outputs.size())\n",
    "\n",
    "        loss = loss_fn(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backpropagate the gradients\n",
    "\n",
    "        optimizer.step()  # Adjust learning weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate the average loss per batch and append to the list for plotting\n",
    "        last_loss = running_loss / (i + 1)\n",
    "        loss_values.append(last_loss)\n",
    "\n",
    "        print('  batch {} loss: {:.4f}'.format(i + 1, last_loss))\n",
    "\n",
    "    return loss_values\n",
    "\n",
    "# Call the training function and store the loss values\n",
    "loss_values = train_one_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss\n",
    "plt.plot(loss_values)\n",
    "plt.title('Training Loss per Batch')\n",
    "plt.xlabel('Batch number')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
